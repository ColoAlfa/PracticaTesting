{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification.py",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNic+UvOmQiCYZGOUoWU5a9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColoAlfa/PracticaTesting/blob/master/TextClassification_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT_G65ctNmYT"
      },
      "source": [
        "Text classification project done by Didac Colominas Abalde, a student at the EPS of the UDL in the field of computing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xegfUHkMjud"
      },
      "source": [
        "# **INTRODUCTION TO THE PROJECT**\n",
        "This notebook trains a sentiment analysis model to classify movie reviews as positive or negative, depending on the text of the review. This type of analysis is called binary classification, or two-class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxkexYznNLKK",
        "outputId": "14eefea5-750e-4675-d30d-4c87774f1bc1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "#Comprobamos que version tenemos de tensorflow\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktq6Ny01NYry"
      },
      "source": [
        "# **STRUCTURE OF DATASET**\n",
        "Now we will proceed to download and view the DATASET. The set has both positive and negative reviews, and it is balanced in such a way that it has the same positive as negative.\n",
        "\n",
        "\n",
        "> We proceed to download the DATASET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ1_VBL6OUKA",
        "outputId": "5854bcf6-141d-4488-e460-47e8bb76692d"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6h8QkSTOWEE"
      },
      "source": [
        "\n",
        "\n",
        "> Now we will see the structure of the dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x0IGOdKPXX-",
        "outputId": "b9679ab6-f2a4-4125-ff68-96e268954cc4"
      },
      "source": [
        "os.listdir(dataset_dir)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'README', 'train', 'imdb.vocab', 'imdbEr.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyR9XDddP40L",
        "outputId": "f8473082-8ebd-49cf-cb5e-c83946f0fb74"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg',\n",
              " 'urls_neg.txt',\n",
              " 'urls_unsup.txt',\n",
              " 'unsupBow.feat',\n",
              " 'pos',\n",
              " 'urls_pos.txt',\n",
              " 'labeledBow.feat',\n",
              " 'unsup']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxbdtboeP_qt"
      },
      "source": [
        "\n",
        "\n",
        "The positive reviews are in the directory aclImdb/train/pos and the negatives are in aclImdb/train/neg. I will open a random review.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQtEdgwBQ95u",
        "outputId": "635b601d-d5d9-498e-9159-a3b7e6b3e0b6"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/0_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp-nY8NzRaq-"
      },
      "source": [
        "# **DATASET**\n",
        "Next we will prepare the data format to be able to train. For this we will use:\n",
        "\n",
        "\n",
        "# tf.keras.preprocessing.text_dataset_from_directory:\n",
        "Basically what it does is generate a dataset already mounted and ready to use.\n",
        "For this, the directory structure must be:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZgSTb_3S1Jp"
      },
      "source": [
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yicZbBvTTOJ-"
      },
      "source": [
        "Therefore, we must eliminate all those directories that are not aclImdb / train / pos and aclImdb / train / neg, which are the two possible classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BgDrxFsUVOK"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBJrdASUovy"
      },
      "source": [
        "Now we can use text_dataset_from_directory to create a tractable dataset. It is recommended to have 3 subsets of data: **TRAINING**,  **VALIDATION**  and **TEST** .\n",
        "\n",
        "* **Training** : Subset of data used to train the model.\n",
        "* **Validation** : Subset of data used to adjust hyperparameters. It would be like adjusting the dividing line.\n",
        "* **Test** : Subset that is passed when the training and validation test has already been passed,\n",
        "\n",
        "But IMDB lacks a validation set, so we will use the validation_split function. We will take the 25,000 of training, and we will take 80% for a new training pack called \"training\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEC--6XaYcug",
        "outputId": "d16c5722-f8fa-42b0-cc65-1473bf7f6d56"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGLse3tRam-y"
      },
      "source": [
        "Now we will take the remaining in the \"train\" folder for validation. I will already define the test ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqqXqhynjyyw",
        "outputId": "e04daedf-a6b3-48a4-89a3-c048e3128497"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrVtqcK5kK4g",
        "outputId": "3c3e4a10-1391-4767-c79e-f77f77e463f8"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAEFH7pPmtqz"
      },
      "source": [
        "#**STANDARDIZE DATA FOR TRAINING**\n",
        "Standardization refers to removing punctuation or HTML elements to simplify the data set. There are two important terms:\n",
        "\n",
        "\n",
        "\n",
        "*   Tokenization: Separate a phrase, for example, into words (spaces).\n",
        "*   Vectorization: converting the tokens into numbers to be able to enter the neural network.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMtq0-CruWUM"
      },
      "source": [
        "Ahora vamos a por una parte minimizar el texto, y eliminar el codigo HTML.La función de abajo lo que hara es sustituir el \"br />\" por espacios \" \""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vH2lWoFui-I"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFfcs2e01PMU"
      },
      "source": [
        "Now we are going to proceed with Vectorization, for this we will use the imported TextVectorization method. Set an \"output_mode\" to give each token integer values. And then we will call \"adapt\" which will adapt the data set, making the model create an index of strings to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4YeJ72h1z-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d25f96c-40ae-477e-a059-ce91ce825950"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review tf.Tensor(b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.', shape=(), dtype=string)\n",
            "Label neg\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[1287,  313, 2380,  313,  661,    7,    2,   52,  229,    5,    2,\n",
            "         200,    3,   38,  170,  669,   29, 5492,    6,    2,   83,  297,\n",
            "         549,   32,  410,    3,    2,  186,   12,   29,    4,    1,  191,\n",
            "         510,  549,    6,    2, 8229,  212,   46,  576,  175,  168,   20,\n",
            "           1, 5361,  290,    4,    1,  761,  969,    1,    3,   24,  935,\n",
            "        2271,  393,    7,    1, 1675,    4, 3747,  250,  148,    4,  112,\n",
            "         436,  761, 3529,  548,    4, 3633,   31,    2, 1331,   28, 2096,\n",
            "           3, 2912,    9,    6,  163,    4, 1006,   20,    2,    1,   15,\n",
            "          85,   53,  147,    9,  292,   89,  959, 2314,  984,   27,  762,\n",
            "           6,  959,    9,  564,   18,    7, 2140,   32,   24, 1254,   36,\n",
            "           1,   85,    3, 3298,   85,    6, 1410,    3, 1936,    2, 3408,\n",
            "         301,  965,    7,    4,  112,  740, 1977,   12,    1, 2014, 2772,\n",
            "           3,    4,  428,    3, 5177,    6,  512, 1254,    1,  278,   27,\n",
            "         139,   25,  308,    1,  579,    5,  259, 3529,    7,   92, 8981,\n",
            "          32,    2, 3842,  230,   27,  289,    9,   35,    2, 5712,   18,\n",
            "          27,  144, 2166,   56,    6,   26,   46,  466, 2014,   27,   40,\n",
            "        2745,  657,  212,    4, 1376, 3002, 7080,  183,   36,  180,   52,\n",
            "         920,    8,    2, 4028,   12,  969,    1,  158,   71,   53,   67,\n",
            "          85, 2754,    4,  734,   51,    1, 1611,  294,   85,    6,    2,\n",
            "        1164,    6,  163,    4, 3408,   15,   85,    6,  717,   85,   44,\n",
            "           5,   24, 7158,    3,   48,  604,    7,   11,  225,  384,   73,\n",
            "          65,   21,  242,   18,   27,  120,  295,    6,   26,  667,  129,\n",
            "        4028,  948,    6,   67,   48,  158,   93,    1]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrS32-LikSwS"
      },
      "source": [
        "Now we have changed every token for an integer, in case we want to get the string we have the function .get_vocabulary() for get the string again.\n",
        "*`print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])`* That will be Silent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_O9ri-lmJqE"
      },
      "source": [
        "# **OPTIMIZATION FOR INPUT/OUTPUT**\n",
        "\n"
      ]
    }
  ]
}