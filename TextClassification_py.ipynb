{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification.py",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAZKM8boW+EIxYFH1f5NH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColoAlfa/PracticaTesting/blob/master/TextClassification_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT_G65ctNmYT"
      },
      "source": [
        "Proyecto de classificación de texto hecho por Didac Colominas Abalde, estudiante de la EPS de la UDL en la rama de computación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xegfUHkMjud"
      },
      "source": [
        "# **INTRODUCCIÓN AL PROYECTO**\n",
        "Este cuaderno entrena un modelo de análisis de sentimientos para clasificar las reseñas de películas como positivas o negativas , según el texto de la reseña.Este tipo de analisis se llaman de clasificación binaria, o de dos clases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxkexYznNLKK",
        "outputId": "393929f0-168f-41f1-97c0-46a2fa9ba82f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "#Comprobamos que version tenemos de tensorflow\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktq6Ny01NYry"
      },
      "source": [
        "# **DATASET**\n",
        "Ahora procederemos a descargar y visualizar el DATASET. El set tiene tanto reviews positivas como negativas, i esta balanceado de manera que tiene las mismas positivas que negativas.\n",
        "\n",
        "\n",
        "\n",
        "> Procedemos a descargar el DATASET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ1_VBL6OUKA",
        "outputId": "ae3aed86-f6f8-44b5-83b6-2e75bbaa90eb"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6h8QkSTOWEE"
      },
      "source": [
        "\n",
        "\n",
        "> Ahora veremos la estructura del dataset un poco:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x0IGOdKPXX-",
        "outputId": "c5e4e853-1e9c-4132-c4b9-e16dfceaa073"
      },
      "source": [
        "os.listdir(dataset_dir)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdbEr.txt', 'test', 'README', 'imdb.vocab', 'train']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyR9XDddP40L",
        "outputId": "3dff4f8e-607d-4f28-e034-2ae1c77d9cc8"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos',\n",
              " 'unsup',\n",
              " 'labeledBow.feat',\n",
              " 'urls_unsup.txt',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'urls_neg.txt',\n",
              " 'neg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxbdtboeP_qt"
      },
      "source": [
        "\n",
        "\n",
        "Las reseñas positivas se encuentran en aclImdb/train/pos mientras que las negativas se encuentran en  aclImdb/train/neg. Probaremos a abrir cualquiera de las reviews.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQtEdgwBQ95u",
        "outputId": "da0146a5-ab0f-4e9c-fa23-9b49e61cfb5a"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/0_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp-nY8NzRaq-"
      },
      "source": [
        "# **CONJUNTO DE DATOS**\n",
        "A continuación prepararemos el formato de datos para poder entrenar. Para ello usaremos:\n",
        "\n",
        "\n",
        "# tf.keras.preprocessing.text_dataset_from_directory:\n",
        "Basicamente lo que hace es generar un dataset ya montado y listo para usarse. \n",
        "Para ello la estructura del directorio debe ser: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZgSTb_3S1Jp"
      },
      "source": [
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yicZbBvTTOJ-"
      },
      "source": [
        "Por lo tanto deberemos eliminar todos aquellos directorios que no sean aclImdb/train/pos y aclImdb/train/neg que son las dos classes possibles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BgDrxFsUVOK"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBJrdASUovy"
      },
      "source": [
        "Ahora ya podemos usar text_dataset_from_directory para crear un dataset tratable. Se recomienda tener 3 subconjuntos de datos: de **ENTRENAMIENTO**, de **VALIDACIÓN** y de **PRUEBA**.\n",
        "\n",
        "*   **Entrenamiento**: Subconjunto de datos que se usa para entrenar el modelo.\n",
        "*   **Validación**: Subconjunto de datos que se usa para ajustar los hiperparametros. Seria como ajustar la linea divisoria.\n",
        "*   **Prueba**: Subconjunto que se pasa cuando ya se han pasado el de entrenamiento y validacion,\n",
        "\n",
        "Pero IMDB carece de set de validación, por lo que usaremos la función validation_split. Cogeremos los 25.000 de entrenamiento, y cogeremos el 80% para una nueva carpreta de entrenamiento llamada \"training\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEC--6XaYcug",
        "outputId": "4d88438a-5904-4fcd-b392-4ea1f4482f82"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGLse3tRam-y"
      },
      "source": [
        "Ahora de los restantes en la carpeta \"train\" cogeremos para validation. I ya definiremos los de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqqXqhynjyyw",
        "outputId": "36dc4c71-8495-4df3-f43e-018d6461ed3b"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrVtqcK5kK4g",
        "outputId": "9e8c00fe-31b1-4ffc-a640-1fda20fd1084"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAEFH7pPmtqz"
      },
      "source": [
        "# **ESTANDARIZAR DATOS PARA EL ENTRENAMIENTO**\n",
        "La estandaritzación se refiere a eliminar la puntuación o elementos HTML para simplificar el conjunto de datos. Hay dos terminos importantes:\n",
        "*   **Tokenización**: Separar una frase por ejemplo en palabras (espacios).\n",
        "*   **Vectorización**: convertir los tokens en números para poder introducirse a  la red neuronal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMtq0-CruWUM"
      },
      "source": [
        "Ahora vamos a por una parte minimizar el texto, y eliminar el codigo HTML.La función de abajo lo que hara es sustituir el \"br />\" por espacios \" \""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vH2lWoFui-I"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}